{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras_radam import RAdam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [-0.02861677 -0.35610364  0.08545073  0.44196761] 1.0 False {}\n",
      "200 [ 0.03261811  0.22717525 -0.01794826 -0.29578712] 1.0 False {}\n",
      "300 [-0.05142291 -1.20314819  0.06741883  1.67818845] 1.0 False {}\n",
      "400 [ 0.02524344  0.39437841 -0.01845341 -0.56345756] 1.0 False {}\n",
      "500 [-0.01393557  0.14972083  0.13178496  0.30214833] 1.0 False {}\n",
      "600 [-0.00940932 -0.21794437 -0.07053286 -0.078053  ] 1.0 False {}\n",
      "700 [ 0.15933754  0.6436702  -0.21252467 -1.2486151 ] 1.0 True {}\n",
      "800 [ 0.02076679  0.76194649 -0.11274565 -1.48048368] 1.0 False {}\n",
      "900 [-0.02197701  0.36115349 -0.0666353  -0.72957359] 1.0 False {}\n",
      "1000 [-0.03097604  0.54892148  0.00409361 -0.86426983] 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "# test observation cartpole\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "observation = env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "    env.render()\n",
    "    action = env.action_space.sample() # your agent here (this takes random actions)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    if (i+1)%100 == 0:\n",
    "        print(i+1, observation, reward, done, info)\n",
    "        \n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env create\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# make model \n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=(4,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='linear')) # 0 or 1\n",
    "\n",
    "model.compile(loss='mse', optimizer=RAdam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 112.00 264.00\" width=\"112pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-260 108,-260 108,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2107705298560 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2107705298560</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 104,-182.5 104,-146.5 0,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"52\" y=\"-160.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 2107704830776 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2107704830776</title>\n",
       "<polygon fill=\"none\" points=\"0,-73.5 0,-109.5 104,-109.5 104,-73.5 0,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"52\" y=\"-87.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 2107705298560&#45;&gt;2107704830776 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>2107705298560-&gt;2107704830776</title>\n",
       "<path d=\"M52,-146.313C52,-138.289 52,-128.547 52,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"55.5001,-119.529 52,-109.529 48.5001,-119.529 55.5001,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2107896590520 -->\n",
       "<g class=\"node\" id=\"node3\"><title>2107896590520</title>\n",
       "<polygon fill=\"none\" points=\"0,-0.5 0,-36.5 104,-36.5 104,-0.5 0,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"52\" y=\"-14.8\">dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 2107704830776&#45;&gt;2107896590520 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>2107704830776-&gt;2107896590520</title>\n",
       "<path d=\"M52,-73.3129C52,-65.2895 52,-55.5475 52,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"55.5001,-46.5288 52,-36.5288 48.5001,-46.5289 55.5001,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2107713579048 -->\n",
       "<g class=\"node\" id=\"node4\"><title>2107713579048</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 104,-255.5 104,-219.5 0,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"52\" y=\"-233.8\">2107713579048</text>\n",
       "</g>\n",
       "<!-- 2107713579048&#45;&gt;2107705298560 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2107713579048-&gt;2107705298560</title>\n",
       "<path d=\"M52,-219.313C52,-211.289 52,-201.547 52,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"55.5001,-192.529 52,-182.529 48.5001,-192.529 55.5001,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.26364708, -0.4174099 ]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect Q\n",
    "s = np.array([1,0,1,0])\n",
    "\n",
    "model.predict(s.reshape(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight init\n",
    "ws = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "memory = collections.deque(maxlen = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(32):\n",
    "    action = env.action_space.sample() # np.random.randint(2)\n",
    "    s2, r, done, _ = env.step(action)\n",
    "    \n",
    "    memory.append([s, action, r, done, s2])\n",
    "    \n",
    "    s = env.reset() if done else s2 # done 이면 리셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.02260046, -0.01159818,  0.01786278,  0.00580626]),\n",
       " 1,\n",
       " 1.0,\n",
       " False,\n",
       " array([ 0.0223685 ,  0.1832631 ,  0.01797891, -0.28118764])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[array([-0.154694  , -1.36514973,  0.1639523 ,  1.80262286]),\n",
       " 0,\n",
       " 1.0,\n",
       " False,\n",
       " array([-0.181997  , -1.56168056,  0.20000476,  2.14145217])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(memory[0], memory[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0 # 처음에는 탐험만 수행한다. (랜덤 행동)\n",
    "gamma = 0.99 # 감쇠율 (discount factor, 미래 보상을 얼마나 중요시할 지를 결정)\n",
    "returns = [] # 에피소드 당 총보상값을 저장한다\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: 14, Epsilon: 0.99871\n",
      "Episode: 2, Reward: 11, Epsilon: 0.99763\n",
      "Episode: 3, Reward: 37, Epsilon: 0.99398\n",
      "Episode: 4, Reward: 24, Epsilon: 0.99162\n",
      "Episode: 5, Reward: 19, Epsilon: 0.98976\n",
      "Episode: 6, Reward: 12, Epsilon: 0.98858\n",
      "Episode: 7, Reward: 14, Epsilon: 0.98721\n",
      "Episode: 8, Reward: 30, Epsilon: 0.98429\n",
      "Episode: 9, Reward: 15, Epsilon: 0.98283\n",
      "Episode: 10, Reward: 14, Epsilon: 0.98146\n",
      "Episode: 11, Reward: 12, Epsilon: 0.98030\n",
      "Episode: 12, Reward: 24, Epsilon: 0.97797\n",
      "Episode: 13, Reward: 17, Epsilon: 0.97633\n",
      "Episode: 14, Reward: 12, Epsilon: 0.97517\n",
      "Episode: 15, Reward: 10, Epsilon: 0.97421\n",
      "Episode: 16, Reward: 16, Epsilon: 0.97266\n",
      "Episode: 17, Reward: 29, Epsilon: 0.96988\n",
      "Episode: 18, Reward: 26, Epsilon: 0.96738\n",
      "Episode: 19, Reward: 30, Epsilon: 0.96452\n",
      "Episode: 20, Reward: 9, Epsilon: 0.96366\n",
      "Episode: 21, Reward: 37, Epsilon: 0.96014\n",
      "Episode: 22, Reward: 15, Epsilon: 0.95871\n",
      "Episode: 23, Reward: 16, Epsilon: 0.95719\n",
      "Episode: 24, Reward: 43, Epsilon: 0.95313\n",
      "Episode: 25, Reward: 35, Epsilon: 0.94984\n",
      "Episode: 26, Reward: 16, Epsilon: 0.94833\n",
      "Episode: 27, Reward: 18, Epsilon: 0.94665\n",
      "Episode: 28, Reward: 34, Epsilon: 0.94347\n",
      "Episode: 29, Reward: 12, Epsilon: 0.94235\n",
      "Episode: 30, Reward: 23, Epsilon: 0.94020\n",
      "Episode: 31, Reward: 44, Epsilon: 0.93612\n",
      "Episode: 32, Reward: 26, Epsilon: 0.93372\n",
      "Episode: 33, Reward: 17, Epsilon: 0.93215\n",
      "Episode: 34, Reward: 51, Epsilon: 0.92746\n",
      "Episode: 35, Reward: 23, Epsilon: 0.92535\n",
      "Episode: 36, Reward: 34, Epsilon: 0.92224\n",
      "Episode: 37, Reward: 16, Epsilon: 0.92078\n",
      "Episode: 38, Reward: 30, Epsilon: 0.91805\n",
      "Episode: 39, Reward: 30, Epsilon: 0.91533\n",
      "Episode: 40, Reward: 11, Epsilon: 0.91434\n",
      "Episode: 41, Reward: 12, Epsilon: 0.91326\n",
      "Episode: 42, Reward: 37, Epsilon: 0.90992\n",
      "Episode: 43, Reward: 31, Epsilon: 0.90713\n",
      "Episode: 44, Reward: 27, Epsilon: 0.90471\n",
      "Episode: 45, Reward: 19, Epsilon: 0.90302\n",
      "Episode: 46, Reward: 15, Epsilon: 0.90168\n",
      "Episode: 47, Reward: 22, Epsilon: 0.89972\n",
      "Episode: 48, Reward: 17, Epsilon: 0.89821\n",
      "Episode: 49, Reward: 15, Epsilon: 0.89688\n",
      "Episode: 50, Reward: 29, Epsilon: 0.89431\n",
      "Episode: 51, Reward: 22, Epsilon: 0.89236\n",
      "Episode: 52, Reward: 30, Epsilon: 0.88972\n",
      "Episode: 53, Reward: 21, Epsilon: 0.88788\n",
      "Episode: 54, Reward: 23, Epsilon: 0.88586\n",
      "Episode: 55, Reward: 25, Epsilon: 0.88367\n",
      "Episode: 56, Reward: 15, Epsilon: 0.88236\n",
      "Episode: 57, Reward: 13, Epsilon: 0.88123\n",
      "Episode: 58, Reward: 12, Epsilon: 0.88018\n",
      "Episode: 59, Reward: 14, Epsilon: 0.87897\n",
      "Episode: 60, Reward: 11, Epsilon: 0.87801\n",
      "Episode: 61, Reward: 18, Epsilon: 0.87645\n",
      "Episode: 62, Reward: 38, Epsilon: 0.87316\n",
      "Episode: 63, Reward: 11, Epsilon: 0.87222\n",
      "Episode: 64, Reward: 17, Epsilon: 0.87075\n",
      "Episode: 65, Reward: 13, Epsilon: 0.86963\n",
      "Episode: 66, Reward: 45, Epsilon: 0.86577\n",
      "Episode: 67, Reward: 22, Epsilon: 0.86389\n",
      "Episode: 68, Reward: 16, Epsilon: 0.86253\n",
      "Episode: 69, Reward: 26, Epsilon: 0.86031\n",
      "Episode: 70, Reward: 9, Epsilon: 0.85955\n",
      "Episode: 71, Reward: 29, Epsilon: 0.85709\n",
      "Episode: 72, Reward: 17, Epsilon: 0.85565\n",
      "Episode: 73, Reward: 25, Epsilon: 0.85354\n",
      "Episode: 74, Reward: 13, Epsilon: 0.85244\n",
      "Episode: 75, Reward: 28, Epsilon: 0.85009\n",
      "Episode: 76, Reward: 20, Epsilon: 0.84841\n",
      "Episode: 77, Reward: 43, Epsilon: 0.84481\n",
      "Episode: 78, Reward: 16, Epsilon: 0.84348\n",
      "Episode: 79, Reward: 17, Epsilon: 0.84206\n",
      "Episode: 80, Reward: 21, Epsilon: 0.84031\n",
      "Episode: 81, Reward: 24, Epsilon: 0.83832\n",
      "Episode: 82, Reward: 13, Epsilon: 0.83725\n",
      "Episode: 83, Reward: 15, Epsilon: 0.83601\n",
      "Episode: 84, Reward: 30, Epsilon: 0.83353\n",
      "Episode: 85, Reward: 15, Epsilon: 0.83230\n",
      "Episode: 86, Reward: 42, Epsilon: 0.82885\n",
      "Episode: 87, Reward: 10, Epsilon: 0.82803\n",
      "Episode: 88, Reward: 11, Epsilon: 0.82714\n",
      "Episode: 89, Reward: 21, Epsilon: 0.82542\n",
      "Episode: 90, Reward: 16, Epsilon: 0.82412\n",
      "Episode: 91, Reward: 9, Epsilon: 0.82339\n",
      "Episode: 92, Reward: 9, Epsilon: 0.82265\n",
      "Episode: 93, Reward: 9, Epsilon: 0.82192\n",
      "Episode: 94, Reward: 13, Epsilon: 0.82087\n",
      "Episode: 95, Reward: 20, Epsilon: 0.81925\n",
      "Episode: 96, Reward: 13, Epsilon: 0.81820\n",
      "Episode: 97, Reward: 14, Epsilon: 0.81707\n",
      "Episode: 98, Reward: 43, Epsilon: 0.81360\n",
      "Episode: 99, Reward: 16, Epsilon: 0.81232\n",
      "Episode: 100, Reward: 13, Epsilon: 0.81128\n",
      "Episode: 101, Reward: 28, Epsilon: 0.80904\n",
      "Episode: 102, Reward: 13, Epsilon: 0.80800\n",
      "Episode: 103, Reward: 12, Epsilon: 0.80704\n",
      "Episode: 104, Reward: 14, Epsilon: 0.80592\n",
      "Episode: 105, Reward: 19, Epsilon: 0.80441\n",
      "Episode: 106, Reward: 9, Epsilon: 0.80370\n",
      "Episode: 107, Reward: 12, Epsilon: 0.80275\n",
      "Episode: 108, Reward: 35, Epsilon: 0.79998\n",
      "Episode: 109, Reward: 10, Epsilon: 0.79919\n",
      "Episode: 110, Reward: 19, Epsilon: 0.79769\n",
      "Episode: 111, Reward: 12, Epsilon: 0.79675\n",
      "Episode: 112, Reward: 13, Epsilon: 0.79572\n",
      "Episode: 113, Reward: 11, Epsilon: 0.79486\n",
      "Episode: 114, Reward: 35, Epsilon: 0.79212\n",
      "Episode: 115, Reward: 10, Epsilon: 0.79134\n",
      "Episode: 116, Reward: 13, Epsilon: 0.79032\n",
      "Episode: 117, Reward: 15, Epsilon: 0.78915\n",
      "Episode: 118, Reward: 12, Epsilon: 0.78822\n",
      "Episode: 119, Reward: 43, Epsilon: 0.78488\n",
      "Episode: 120, Reward: 8, Epsilon: 0.78426\n",
      "Episode: 121, Reward: 15, Epsilon: 0.78310\n",
      "Episode: 122, Reward: 21, Epsilon: 0.78148\n",
      "Episode: 123, Reward: 17, Epsilon: 0.78017\n",
      "Episode: 124, Reward: 12, Epsilon: 0.77924\n",
      "Episode: 125, Reward: 12, Epsilon: 0.77832\n",
      "Episode: 126, Reward: 20, Epsilon: 0.77678\n",
      "Episode: 127, Reward: 58, Epsilon: 0.77235\n",
      "Episode: 128, Reward: 15, Epsilon: 0.77121\n",
      "Episode: 129, Reward: 48, Epsilon: 0.76756\n",
      "Episode: 130, Reward: 41, Epsilon: 0.76446\n",
      "Episode: 131, Reward: 15, Epsilon: 0.76333\n",
      "Episode: 132, Reward: 45, Epsilon: 0.75995\n",
      "Episode: 133, Reward: 14, Epsilon: 0.75890\n",
      "Episode: 134, Reward: 25, Epsilon: 0.75703\n",
      "Episode: 135, Reward: 26, Epsilon: 0.75509\n",
      "Episode: 136, Reward: 31, Epsilon: 0.75278\n",
      "Episode: 137, Reward: 38, Epsilon: 0.74997\n",
      "Episode: 138, Reward: 18, Epsilon: 0.74864\n",
      "Episode: 139, Reward: 14, Epsilon: 0.74760\n",
      "Episode: 140, Reward: 60, Epsilon: 0.74319\n",
      "Episode: 141, Reward: 51, Epsilon: 0.73946\n",
      "Episode: 142, Reward: 20, Epsilon: 0.73800\n",
      "Episode: 143, Reward: 39, Epsilon: 0.73517\n",
      "Episode: 144, Reward: 43, Epsilon: 0.73206\n",
      "Episode: 145, Reward: 29, Epsilon: 0.72997\n",
      "Episode: 146, Reward: 19, Epsilon: 0.72860\n",
      "Episode: 147, Reward: 14, Epsilon: 0.72759\n",
      "Episode: 148, Reward: 18, Epsilon: 0.72630\n",
      "Episode: 149, Reward: 33, Epsilon: 0.72394\n",
      "Episode: 150, Reward: 20, Epsilon: 0.72252\n",
      "Episode: 151, Reward: 25, Epsilon: 0.72074\n",
      "Episode: 152, Reward: 51, Epsilon: 0.71712\n",
      "Episode: 153, Reward: 42, Epsilon: 0.71416\n",
      "Episode: 154, Reward: 103, Epsilon: 0.70694\n",
      "Episode: 155, Reward: 15, Epsilon: 0.70590\n",
      "Episode: 156, Reward: 42, Epsilon: 0.70298\n",
      "Episode: 157, Reward: 87, Epsilon: 0.69698\n",
      "Episode: 158, Reward: 52, Epsilon: 0.69342\n",
      "Episode: 159, Reward: 46, Epsilon: 0.69028\n",
      "Episode: 160, Reward: 26, Epsilon: 0.68851\n",
      "Episode: 161, Reward: 49, Epsilon: 0.68520\n",
      "Episode: 162, Reward: 22, Epsilon: 0.68371\n",
      "Episode: 163, Reward: 14, Epsilon: 0.68277\n",
      "Episode: 164, Reward: 20, Epsilon: 0.68143\n",
      "Episode: 165, Reward: 55, Epsilon: 0.67774\n",
      "Episode: 166, Reward: 73, Epsilon: 0.67289\n",
      "Episode: 167, Reward: 31, Epsilon: 0.67084\n",
      "Episode: 168, Reward: 47, Epsilon: 0.66774\n",
      "Episode: 169, Reward: 50, Epsilon: 0.66446\n",
      "Episode: 170, Reward: 47, Epsilon: 0.66139\n",
      "Episode: 171, Reward: 39, Epsilon: 0.65885\n",
      "Episode: 172, Reward: 38, Epsilon: 0.65639\n",
      "Episode: 173, Reward: 42, Epsilon: 0.65368\n",
      "Episode: 174, Reward: 23, Epsilon: 0.65220\n",
      "Episode: 175, Reward: 91, Epsilon: 0.64639\n",
      "Episode: 176, Reward: 34, Epsilon: 0.64423\n",
      "Episode: 177, Reward: 16, Epsilon: 0.64321\n",
      "Episode: 178, Reward: 120, Epsilon: 0.63566\n",
      "Episode: 179, Reward: 45, Epsilon: 0.63285\n",
      "Episode: 180, Reward: 36, Epsilon: 0.63061\n",
      "Episode: 181, Reward: 78, Epsilon: 0.62579\n",
      "Episode: 182, Reward: 57, Epsilon: 0.62229\n",
      "Episode: 183, Reward: 26, Epsilon: 0.62070\n",
      "Episode: 184, Reward: 59, Epsilon: 0.61711\n",
      "Episode: 185, Reward: 102, Epsilon: 0.61095\n",
      "Episode: 186, Reward: 53, Epsilon: 0.60777\n",
      "Episode: 187, Reward: 52, Epsilon: 0.60467\n",
      "Episode: 188, Reward: 44, Epsilon: 0.60206\n",
      "Episode: 189, Reward: 109, Epsilon: 0.59564\n",
      "Episode: 190, Reward: 81, Epsilon: 0.59092\n",
      "Episode: 191, Reward: 65, Epsilon: 0.58715\n",
      "Episode: 192, Reward: 41, Epsilon: 0.58479\n",
      "Episode: 193, Reward: 46, Epsilon: 0.58215\n",
      "Episode: 194, Reward: 15, Epsilon: 0.58129\n",
      "Episode: 195, Reward: 93, Epsilon: 0.57601\n",
      "Episode: 196, Reward: 113, Epsilon: 0.56965\n",
      "Episode: 197, Reward: 47, Epsilon: 0.56702\n",
      "Episode: 198, Reward: 30, Epsilon: 0.56535\n",
      "Episode: 199, Reward: 124, Epsilon: 0.55851\n",
      "Episode: 200, Reward: 75, Epsilon: 0.55441\n",
      "Episode: 201, Reward: 78, Epsilon: 0.55018\n",
      "Episode: 202, Reward: 31, Epsilon: 0.54851\n",
      "Episode: 203, Reward: 110, Epsilon: 0.54262\n",
      "Episode: 204, Reward: 130, Epsilon: 0.53574\n",
      "Episode: 205, Reward: 19, Epsilon: 0.53474\n",
      "Episode: 206, Reward: 161, Epsilon: 0.52636\n",
      "Episode: 207, Reward: 123, Epsilon: 0.52005\n",
      "Episode: 208, Reward: 27, Epsilon: 0.51867\n",
      "Episode: 209, Reward: 163, Epsilon: 0.51045\n",
      "Episode: 210, Reward: 150, Epsilon: 0.50300\n",
      "Episode: 211, Reward: 83, Epsilon: 0.49892\n",
      "Episode: 212, Reward: 189, Epsilon: 0.48977\n",
      "Episode: 213, Reward: 154, Epsilon: 0.48244\n",
      "Episode: 214, Reward: 169, Epsilon: 0.47452\n",
      "Episode: 215, Reward: 92, Epsilon: 0.47027\n",
      "Episode: 216, Reward: 168, Epsilon: 0.46260\n",
      "Episode: 217, Reward: 134, Epsilon: 0.45657\n",
      "Episode: 218, Reward: 200, Epsilon: 0.44773\n"
     ]
    }
   ],
   "source": [
    "# 총 보상이 200 이상이 되면 그만 학습한다.\n",
    "for episode in range(300):\n",
    "    \n",
    "    total_reward = 0 # 에피소스당 총보상값\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for i in range(200): # 한 에피소드당 최대 200번만 행동한다\n",
    "        \n",
    "        ### 탐험 확률 지정 (1 부터 시작해서 점점 낮아지다 최소값은 1%)\n",
    "        epsilon = 0.01 + (1-0.01)*np.exp(-0.0001*count)\n",
    "        count += 1\n",
    "        \n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample() # 랜덤 행동\n",
    "        else:\n",
    "            action = np.argmax(model.predict(s.reshape(1,4))[0]) # Q값이 높은 행동 선택\n",
    "            \n",
    "        s2, r, done, _ = env.step(action)\n",
    "        \n",
    "        total_reward += r\n",
    "        \n",
    "        memory.append([s, action, r, done, s2])\n",
    "        \n",
    "        ### 학습 (배치크기는 32)\n",
    "        indices = np.random.choice(len(memory), 32, replace=False)\n",
    "        samples = [memory[i] for i in indices]\n",
    "        \n",
    "        X = np.zeros([32,4]) # 입력 상태값\n",
    "        y = np.zeros([32,2]) # 목표 Q값\n",
    "        \n",
    "        for i, sample in enumerate(samples): # sample -> [s,a,r,done,s2]\n",
    "            X[i] = sample[0]\n",
    "            y[i] = model.predict(sample[0].reshape(1,4))[0]\n",
    "            \n",
    "            if sample[3] == True: # done\n",
    "                y[i][sample[1]] = sample[2]\n",
    "            else:\n",
    "                y[i][sample[1]] = sample[2] + gamma*np.max(model.predict(sample[-1].reshape(1,4))[0])\n",
    "                \n",
    "        model.fit(X, y, epochs=1, verbose=False)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        else:\n",
    "            s = s2\n",
    "            \n",
    "    print('Episode: %d, Reward: %d, Epsilon: %.5f' % (episode+1, total_reward, epsilon))\n",
    "    if total_reward >= 200:\n",
    "        break\n",
    "    returns.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Count: 180\n",
      "[1.57518219 0.652486   0.20982482 0.91238075] 1.0 True {}\n",
      "Epoch: 2, Count: 200\n",
      "[-0.63752195 -0.32772351 -0.014283    0.1001671 ] 1.0 True {'TimeLimit.truncated': True}\n",
      "Epoch: 3, Count: 200\n",
      "[0.47312438 0.41005765 0.06962299 0.02027365] 1.0 True {'TimeLimit.truncated': True}\n"
     ]
    }
   ],
   "source": [
    "# 학습 시킨 모델을 이용하여 cartpole test\n",
    "\n",
    "for epoch in range(3):\n",
    "    env.reset()\n",
    "    \n",
    "    done = False\n",
    "    n = 0\n",
    "    while not done:\n",
    "        #a = np.random.randint(2)\n",
    "        a = np.argmax(model.predict(s.reshape(1,4))[0])\n",
    "        s, r, done, info = env.step(a)\n",
    "        n += 1\n",
    "        env.render()\n",
    "        #print(s, r, done, info)\n",
    "        \n",
    "    print('Epoch: %d, Count: %d' % (epoch+1, n))\n",
    "    print(s, r, done, info)\n",
    "    #input()\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 확인시 생각보다 잘버티는걸 확인할 수 있었습니다. 아마 총 리워드가 천점이상 넘어가면 계속 버티고 있지않을까 예상합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
